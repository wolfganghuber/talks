---
title: ""
title-slide-attributes:
  data-background-image: img/title-page-dolomites.png
  data-background-size: contain
  data-background-opacity: "1"
  data-heading-color: #E84614;
author: Wolfgang Huber
date: 2025-07-22
date-format: iso
format:
  revealjs: 
    theme: [default, wh.scss]
    transition: slide
    scrollable: true
    slide-number: c/t
    show-slide-number: all
    auto-stretch: false
    center-title-slide: false
    logo: img/ubds3-logo.png
  html:
    code-line-numbers: false
    embed-resources: true
execute: 
  echo: false
  warning: true
  error: false
  message: false
slide-level: 1
editor_options: 
  chunk_output_type: console
engine: knitr
knitr:
  opts_chunk: 
    R.options:
      width: 120
---

<!-- NOTE
auto-stretch is a huge source of grief for slide layout. See https://quarto.org/docs/presentations/revealjs/advanced.html, section on stretch
-->

# Analysis of variance, regression and testing
Wolfgang Huber

![](img/title-page-dolomites.png)


# Analysis of variance

A set of observations

```{r}
#| label: two2dpoints-blank
#| echo: false
#| message: false
#| fig-width: 5
#| fig-height: 4
#| out-width: "40%"
library("tidyverse")
library("ggbeeswarm")
set.seed(0xdada)
beta = c(1, 1, 0.2)
f = function(x, gr1, gr2) (beta[3] * x + beta[1] * as.integer(gr1) + beta[2] * as.integer(gr2))

simdat = tibble(
  x = runif(100, min = 0, max = 10),
  gr0 = factor(rep("", length(x))),
  gr1 = factor(sample(c("indolent", "aggresive"), length(x), replace = TRUE)),
  gr2 = factor(sample(c("hot", "cold"), length(x), replace = TRUE)),
  y = rnorm(length(x), mean = f(x, gr1, gr2), sd = 0.3),
)
ylim = range(simdat$y)
sd0 = sd(simdat$y) |> round(1)
p = ggplot(simdat, aes(x = gr0, y = y)) + geom_boxplot() + geom_beeswarm(cex = 3) + xlab("") + ylim(ylim)
p 
```

Standard deviation: `r sd0`.

<br>

### Find an explanatory factor

```{r}
#| label: two2dpoints-onefactor
#| echo: false
#| fig-width: 5
#| fig-height: 4
#| out-width: "40%"
sd1 = group_by(simdat, gr2) |> summarise(v = var(y)) |> (\(x) round(sqrt(mean(x$v)), 1))()
p + facet_grid(cols = vars(gr2))
```

Standard deviation: `r sd1`.

<br>

### And another explanatory factor

```{r}
#| label: two2dpoints-twofactors
#| echo: false
#| fig-width: 5
#| fig-height: 4
#| out-width: "40%"
sd2 = group_by(simdat, gr1, gr2) |> summarise(v = var(y), .groups = "keep") |> (\(x) round(sqrt(mean(x$v)), 1))()
p + facet_grid(rows = vars(gr1), cols = vars(gr2))
```

Standard deviation: `r sd2`.

<br>

### And another explanatory variable, this time continuous-valued

```{r}
#| label: two2dpoints-twofactors+continuous
#| echo: false
#| fig-width: 5
#| fig-height: 4
#| out-width: "40%"
ggplot(simdat, aes(x = x, y = y)) + geom_point(aes(x = x)) + facet_grid(rows = vars(gr1), cols = vars(gr2)) + ylim(ylim)
```

Residuals: $y - \beta_1\,g1 - \beta_2\,g2 - \beta_3\,x$.

```{r}
#| label: two2dpoints-residuals
#| echo: false
#| fig-width: 5
#| fig-height: 4
#| out-width: "40%"
sd3 = with(simdat, sd(y - f(x, gr1, gr2))) |> (\(x) paste(round(x, 1), collapse = ", "))()
ggplot(simdat, aes(x = gr0, y = y - f(x, gr1, gr2))) + geom_beeswarm(cex = 3) + 
  xlab("") + ylim(diff(range(ylim))*c(-0.5,.5))
```

Standard deviation: `r sd3`.

<br>

## Basic idea of ANOVA

$$
\text{Overall variance}  = 
  \text{Signal component 1} + 
  \text{Signal component 2} + 
  \ldots + 
  \text{Rest}  
$$

<br><br><br><br><br><br><br>

# Linear model

$$
 y = \beta_0 + \beta_1\,x_1 + \beta_2\,x_2 + \ldots + \varepsilon 
$$

::: {.fragment}
In vector notation

$$
y = \beta \cdot x + \varepsilon 
$$

where now $x,\beta\in\mathbb{R}^n$ and $x = (1, x_1, x_2, \ldots)$.
:::

::: {.fragment}
If we let $\beta$ be an $m\times n$ matrix, $y$ itself can be vector-valued ($y, \varepsilon \in \mathbb{R}^m$).
:::

<br>

## But how to estimate $\beta$ ? {.fragment}

::: {.fragment}
Basic idea: minimize $\varepsilon$. 
:::

::: {.fragment}
Ordinary least squares: $\Rightarrow$
$$
\hat{\beta} = (x^t\,x)^{-1}\,x^t\,y
$$
:::

::: {.fragment}
Example: two groups, $x$ is either $(1,0)$ or $(1,1)$ $\Rightarrow$ 

 - $\hat{\beta}_0 = \text{mean of 1st group}$
 - $\hat{\beta}_1 = \text{difference between mean of 2nd group and that of 1st}$.
:::

<br>

## The connection to hypothesis testing  {.fragment}

::: {.fragment}
In the two groups example: how sure are we that the __true__ $\beta_1$ is different from zero ?

This is the $t$-test!

(More generally, could ask whether $\beta$ is outside a set $N\subset\mathbb{R}^n$)


:::

# Colinearity, confounding

```{r}
#| label: colinear-1
#| echo: !expr -1
#| message: false
library("MASS")
data("Boston")
head(Boston)
```

Housing Values in Suburbs of Boston.
The `Boston` data frame has 506 rows and 14 columns.

```{r}
#| label: bostonanno
#| echo: false
bostonanno = read.table(textConnection("
varname|meaning
zn|proportion of residential land zoned for lots over 25,000 sq.ft
indus|proportion of non-retail business acres per town
chas|Charles River proximity (1 if tract bounds river; 0 otherwise)
nox|nitrogen oxides concentration (parts per 10 million)
rm|average number of rooms per dwelling
age|proportion of owner-occupied units built prior to 1940
dis|weighted mean of distances to five Boston employment centres
rad|index of accessibility to radial highways
tax|full-value property-tax rate per $10,000
ptratio|pupil-teacher ratio by town
black|$1000(b−0.63)^2$, where $b$ is the proportion of blacks
lstat|lower status of the population (percent)
medv|median value of owner-occupied homes in $1000s"), sep = "|", header = TRUE)
knitr::kable(dplyr::filter(bostonanno, varname %in% c("nox", "rm", "dis")))
```

```{r}
#| label: colinear-2
#| echo: true
#| message: false
#| fig-width: 5
#| fig-height: 5
#| out-width: "60%"
fit1 = lm(medv ~ dis + rm, data = Boston)
coef(fit1)
fit2 = lm(medv ~ dis + rm + nox, data = Boston)
coef(fit2)

library("GGally")
ggpairs(dplyr::select(Boston, nox, dis, rm))

library("corrplot")
corrplot(cor(Boston), method = "circle", type = "upper", tl.cex = 0.7)
```
```{r}
#| label: colinear-3
#| echo: false
knitr::kable(bostonanno)
```

# Blocking, pairing

```{r}
#| label: pairing-1
#| message: false
#| fig-width: 3
#| fig-height: 3
#| out-width: "40%"
library("BSDA")
data("Fitness")
ggplot(Fitness, aes(x = test, y = number)) + geom_beeswarm(size = 3)
t.test(number ~ test, data = Fitness)
```
**The p value is not "significant".**

```{r}
#| label: pairing-2
fit = lm(number ~ test + subject, data = Fitness)
summary(fit)
```

**The p value for `testBefore` (=difference before - after) is "significant".**

```{r}
#| label: pairing-3
#| fig-width: 3
#| fig-height: 3
#| out-width: "40%"
FitnessW = tidyr::spread(Fitness, test, number) 
t.test(FitnessW$After, FitnessW$Before, paired = TRUE)
ggplot(FitnessW, aes(x = Before, y = After)) + geom_point() + geom_abline(col="blue")
```

```{r}
#| label: attempts at better output
#| eval: false
#| echo: false
c(tt$p.value, tt$estimate)

default_hook_output =  knitr::knit_hooks$get("output")
knitr::knit_hooks$set(output = function(x, options) {
  x <- gsub("p-value", "<span style='color:red; font-weight:bold;'>p-value</span>", x)
  default_hook_output(x, options)
})
```

**The paired $t$-test is the same as the linear model with `subject` as a covariate.**

# Generalizations

$$
y = \beta \cdot x + \varepsilon 
$$

## Logistic regression

```{r}
#| label: logisticregression
#| echo: false
#| fig-width: 5
#| fig-height: 3
#| out-width: "60%"

# From: https://www.rpubs.com/johnakwei/167443
HrsStudying = tibble(
  Hours = c(0.50, 0.75, 1.00, 1.25, 1.50, 1.75, 1.75, 2.00, 2.25, 2.50, 2.75, 3.00, 3.25, 3.50, 4.00, 4.25, 4.50, 4.75, 5.00, 5.50),
  Pass  = c(0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1)
)
ggplot(HrsStudying, aes(Hours, Pass)) + geom_point(aes()) +
  geom_smooth(method = "glm", method.args = list(family="binomial"),
    formula = y ~ x, se = FALSE) +
  labs(x = "Hours Studying", y = "Probability of Passing Exam")
```

$$
\log\frac{p}{1-p} = \beta \cdot x 
$$

<br><br>

## Generalized Linear Models

- Model $\beta \cdot x$, 
- (optional:) transform with a "link function" (e.g., $\exp$), 
- insert the result as parameter into a statistical distribution.

<br><br>

## Splines

(...)

<br><br>

## Gaussian processes

In a Bayesian view of linear regression, we assume the data is generated from a **linear function** with some Gaussian noise:

$$
y = \mathbf{x}^\top \mathbf{w} + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2)
$$
We place a **prior** on the weights,

$$
\mathbf{w} \sim \mathcal{N}(0, \tau^2 I),
$$

and this leads to a **Gaussian prior** over functions $f(\mathbf{x}) = \mathbf{x}^\top \mathbf{w}$. When we marginalize over $\mathbf{w}$, the function values at input points follow a **multivariate Gaussian distribution**.

A Gaussian Process can be seen as an **infinite-dimensional generalization** of the above.

* Instead of explicitly parameterizing functions with a finite weight vector $\mathbf{w}$, we define a **distribution over functions** directly.
* A GP is fully specified by its **mean function** $m(\mathbf{x})$ and **covariance function (kernel)** $k(\mathbf{x}, \mathbf{x}')$.

Formally:

$$
f(\mathbf{x}) \sim \mathcal{GP}(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}'))
$$

In fact, if you choose a **linear kernel**:

$$
k(\mathbf{x}, \mathbf{x}') = \mathbf{x}^\top \mathbf{x}'
$$

then the GP reduces to Bayesian linear regression.

### GPs are more powerful

* By choosing **nonlinear kernels** (e.g. RBF, Matérn), GPs can represent **nonlinear functions** while still retaining the analytical tractability of Gaussian distributions.
* This makes them **nonparametric** models—unlike linear regression which has a fixed number of parameters, GPs can adapt complexity to the data.


# Hypothesis testing — why?

## Test efficacy of a drug on people

- not an experiment — no complete control
- finite sample size

## Prioritise results from a biological high-throughput experiment 

- e.g., RNA-seq differential expression
- CRISPR screen

## Understand impact of humidity on prevalence of leptospirosis

::: {.incremental}
-  No understanding of mechanism involved / needed / desired 
-  Wouldn't we want to use any available understanding or ‘priors'?
:::

::: {.absolute top=100 left=200 width="800"}
![Any fool can work with infinite data. Statisticians do finity](img/sherlock.png){.fragment}
:::


# Fundamental tradeoffs in statistical decision making

![](img/moose.jpg) 

# Basic problem: making binary decision

![value: some useful number computed from the data](img/twoclasses_z.png)

False discovery rate
$$
\text{FDR} = \frac{\text{area shaded in light blue}}{\text{sum of areas to left of vertical bar (light blue + dark red)}}
$$

For this, we need to know:

- blue curve: how is $x$ distributed if no effect
- red curve: how is $x$ distributed if there is effect
- the relative sizes of the blue and the red classes

# Basic problem: making binary decision

![p value: suitably transformed version of the value from the preceding slide](img/twoclasses_p.png)

$$
\text{p} = \frac{\text{area shaded in light blue}}{\text{sum of the blue areas (=1)}}
$$

For this, we need to know:

- how is $x$ distributed if no effect

::: {.absolute top=600 left=300 width="800"}
![](img/pvalue-what-could-go-wrong.png){.fragment}
:::

# 
![](img/2025-HypothesisTesting-Huber_15.gif)
 
| Hypothesis testing                              | Machine Learning                                |
| ----------------------------------------------- | ----------------------------------------------- | 
| Some theory/model and no or few parameters | Lots of free parameters | 
| No training data | Lots of training data |
| More rigid/formulaic | Using multiple variables |
| Regulatory use | ... or objects that are not even traditional variables (kernel methods, DNN) |


# Comparing two groups: the $t$-statistic

For univariate real-valued data

:::: {.columns}

::: {.column width="60%"}

<img src="img/t-test-schematic.png" with="400" style="border: 2px solid grey;">

![](img/t-test-details.png){width="450" fig-align="center"}
:::
::: {.column width="40%"}
![](img/t-test-exampledataTP53.png)
:::

::::

# Parametric Theory vs Simulation

:::: {.columns}

::: {.column width="48%"}
![](img/2025-HypothesisTesting-Huber_19.png)

![](img/2025-HypothesisTesting-Huber_22.png){width="300" fig-align="center"}
:::

::: {.column width="48%"}
![](img/2025-HypothesisTesting-Huber_20.png)

![](img/2025-HypothesisTesting-Huber_21.jpg){width="200" fig-align="center"}
:::
:::

<center>
Discuss pros and contras for each</span>
</center>

# Continuation of this talk

In [Google Slides](https://docs.google.com/presentation/d/13S3A2gWKo0noX0nFIIJIfqKGsUBgTMnGLzLH9D8_c4g/edit?slide=id.p16#slide=id.p16)