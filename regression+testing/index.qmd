---
title: ""
title-slide-attributes:
  data-background-image: img/title-page-dolomites.png
  data-background-size: contain
  data-background-opacity: "1"
  data-heading-color: #E84614;
author: Wolfgang Huber
date: 2025-07-22
date-format: iso
format:
  revealjs: 
    theme: [default, wh.scss]
    transition: slide
    scrollable: true
    slide-number: c/t
    show-slide-number: all
    auto-stretch: false
    center-title-slide: false
    logo: img/ubds3-logo.png
# auto-stretch is a huge source of grief for slide layout, see https://quarto.org/docs/presentations/revealjs/advanced.html --> stretch
  html:
    code-line-numbers: false
execute: 
  echo: false
  warning: true
  error: false
  message: false
slide-level: 1
editor_options: 
  chunk_output_type: console
---

# Analysis of variance, regression and testing
Wolfgang Huber

![](img/title-page-dolomites.png)


# Motivation

## Test efficacy of a drug on people

- not an experiment ‚Äî no complete control
- finite sample size

## Prioritise results from a biological high-throughput experiment 

- e.g., RNA-seq differential expression
- CRISPR screen

## Understand impact of humidity on prevalence of leptospirosis

::: {.incremental}
-  No understanding of mechanism involved / needed / desired 
-  Wouldn't we want to use any available understanding or ‚Äòpriors'?
:::

::: {.absolute top=100 left=200 width="800"}
![Any fool can work with infinite data. Statisticians do finity](img/sherlock.png){.fragment}
:::

# Analysis of variance

Basic idea

\[
Overall variance = Signal component 1 + Signal component 2 + \ldots + Rest  
\]

## just noise: 
2D point cloud
## two groups
## more groups
## univariate linear regression
## multivariate linear regression

# Linear model

## estimating $\beta$

## testing

## colinearity, confounding

## blocking, pairing


# Generalizations

## GLMs, Link functions

## Splines

## Gaussian processes


Yes, a **Gaussian Process (GP)** can indeed be understood as a **generalization or extension of linear regression**, especially when viewed from a **Bayesian perspective**. Here's a breakdown of the connection:

---

### üîß Linear Regression (Bayesian view)

In Bayesian linear regression, we assume the data is generated from a **linear function** with some Gaussian noise:

$$
y = \mathbf{x}^\top \mathbf{w} + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2)
$$

We place a **prior** on the weights:

$$
\mathbf{w} \sim \mathcal{N}(0, \tau^2 I)
$$

This leads to a **Gaussian prior** over functions $f(\mathbf{x}) = \mathbf{x}^\top \mathbf{w}$. When we marginalize over $\mathbf{w}$, the function values at input points follow a **multivariate Gaussian distribution**.

---

### üåÄ Gaussian Process as Infinite Linear Regression

A Gaussian Process can be seen as an **infinite-dimensional generalization** of Bayesian linear regression:

* Instead of explicitly parameterizing functions with a finite weight vector $\mathbf{w}$, we define a **distribution over functions** directly.
* A GP is fully specified by its **mean function** $m(\mathbf{x})$ and **covariance function (kernel)** $k(\mathbf{x}, \mathbf{x}')$.

Formally:

$$
f(\mathbf{x}) \sim \mathcal{GP}(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}'))
$$

In fact, if you choose a **linear kernel**:

$$
k(\mathbf{x}, \mathbf{x}') = \mathbf{x}^\top \mathbf{x}'
$$

then the GP reduces to Bayesian linear regression.

---

### üìà Why GPs are more powerful

* By choosing **nonlinear kernels** (e.g. RBF, Mat√©rn), GPs can represent **nonlinear functions** while still retaining the analytical tractability of Gaussian distributions.
* This makes them **nonparametric** models‚Äîunlike linear regression which has a fixed number of parameters, GPs can adapt complexity to the data.

---

### üå≥ Summary

| Concept       | Linear Regression                               | Gaussian Process                    |
| ------------- | ----------------------------------------------- | ----------------------------------- |
| Function form | Linear in features                              | Can be nonlinear (via kernel)       |
| Parameters    | Finite vector $\mathbf{w}$                      | Infinite-dimensional function space |
| Prior         | Over parameters                                 | Over functions                      |
| Posterior     | Gaussian over weights ‚Üí predictive distribution | Gaussian over functions             |

> So yes ‚Äî a Gaussian Process **extends** linear regression by replacing a finite linear model with a flexible, probabilistic model over functions.


# Fundamental tradeoffs in statistical decision making

![](img/moose.jpg) 


<span style="color:#000000">‚Üêbias                             accuracy‚Üí</span>

<span style="color:#000000">Comes in various guises</span>

<span style="color:#0096ff">Accuracy vs Precision</span>

<span style="color:#0096ff">Bias vs Variance</span>

<span style="color:#0096ff">Model complexity vs overfitting</span>

![](img/2025-HypothesisTesting-Huber_2.png)

![](img/2025-HypothesisTesting-Huber_3.png)

<span style="color:#000000">‚Üê precision            dispersion‚Üí </span>

![](img/2025-HypothesisTesting-Huber_4.png)

![](img/2025-HypothesisTesting-Huber_5.png)

<span style="color:#0096ff">Basic problem: binary decision</span>

![](img/2025-HypothesisTesting-Huber_6.png)

![](img/2025-HypothesisTesting-Huber_7.png)

![](img/2025-HypothesisTesting-Huber_8.png)

<span style="color:#000000">some useful number </span>  <span style="color:#000000"> _x_ </span>  <span style="color:#000000"> computed from the data</span>

![](img/2025-HypothesisTesting-Huber_9.png)

<span style="color:#000000">False discovery rate</span>

<span style="color:#000000">For this\, we need to know:</span>

<span style="color:#000000"> </span>  __the __  <span style="color:#2079b4">blue</span>  __ curve: how is __  <span style="color:#000000"> _x_ </span>  <span style="color:#000000"> </span>  __distributed __  <span style="color:#000000">if </span>  <span style="color:#000000"> __no__ </span>  <span style="color:#000000"> eff</span> ect <span style="color:#000000">\,</span>

<span style="color:#000000"> </span>  __the __  <span style="color:#e31b1b">red</span>  __ curve: how is __  _x_  __ distributed if __  __there is__  __ effect__  <span style="color:#000000">\,</span>

<span style="color:#000000"> the </span>  <span style="color:#000000"> __relative sizes__ </span>  <span style="color:#000000"> of the blue and the red classes\.</span>

<span style="color:#0096ff">Basic problem: binary decision</span>

![](img/2025-HypothesisTesting-Huber_10.png)

![](img/2025-HypothesisTesting-Huber_11.png)

![](img/2025-HypothesisTesting-Huber_12.png)

<span style="color:#000000">another useful number\, computed from </span>  <span style="color:#000000"> _x_ </span>  <span style="color:#000000">:     </span>  <span style="color:#000000"> _p_ </span>

![](img/2025-HypothesisTesting-Huber_13.png)

![](img/2025-HypothesisTesting-Huber_14.png)

<span style="color:#000000">For this\, we need to know:</span>

<span style="color:#000000"> </span>  __the __  <span style="color:#2079b4">blue</span>  __ curve: how is __  <span style="color:#000000"> _x_ </span>  <span style="color:#000000"> </span>  __distributed __  <span style="color:#000000">if </span>  <span style="color:#000000"> __no__ </span>  <span style="color:#000000"> eff</span> ect <span style="color:#000000">\,</span>

<span style="color:#000000"> </span>  __the __  <span style="color:#e31b1b">red</span>  __ curve: how is __  _x_  __ distributed if __  __there is__  __ effect__  <span style="color:#000000">\,</span>

<span style="color:#000000"> the </span>  <span style="color:#000000"> __relative sizes__ </span>  <span style="color:#000000"> of the blue and the red classes\.</span>

<span style="color:#0096ff">Hypothesis testing</span>

<span style="color:#000000">Some theory/model and no or few parameters</span>

<span style="color:#000000">No training data</span>

<span style="color:#000000">More rigid/formulaic</span>

<span style="color:#000000">Regulatory use</span>

# Machine Learning
Lots of free parameters
Lots of training data
Using multiple variables
... or objects that are not even traditional variables (e.g. images)

![](img/2025-HypothesisTesting-Huber_15.gif)

::: {.notes}

 Lady testing tea: claimed to be able to tell whether the tea or the milk was added first to a cup.

:::

![](img/2025-HypothesisTesting-Huber_16.png)

# Comparing a univariate (‚àà‚Ñù) measurement betweentwo groups:		the t-statistic

![](img/2025-HypothesisTesting-Huber_17.png)

![](img/2025-HypothesisTesting-Huber_18.png)

# Parametric Theory vs Simulation

![](img/2025-HypothesisTesting-Huber_19.png)

![](img/2025-HypothesisTesting-Huber_20.png)

<span style="color:#0096ff"> __Q__ </span>  <span style="color:#000000">:</span>

<span style="color:#000000">Discuss pros and contras for each</span>

![](img/2025-HypothesisTesting-Huber_21.jpg)

![](img/2025-HypothesisTesting-Huber_22.png)

# Common misunderstandings about t-statistic and t-distribution

# ‚ÄúIf the data with sample sizes n1 and n2 are identically normal distributed and independent,  then the t-test is optimal, and under H0, the t-statistic follows a t-distribution with ŒΩ=n1+n2-2.‚Äù
These are sufficient, not necessary, conditions.
Deviation from normality: test typically maintains FPR (‚Äútype-I error‚Äù) control. (It just no longer has provably optimal power.)Options: transform data, use permutations, simulations 
Deviation from independence: FPR control is lost, p-values will likely be totally wrong (e.g., for positive correlation, too optimistic).No easy options: ‚Ä¶ simulations that keep the dependence structure (?)‚Ä¶ batch adjustments that remove correlation (?)‚Ä¶ empirical null ‚Äî possible if you have many tests

::: {.notes}

Note that the null hypothesis, and the power of Wilcoxon test are different from the t-test

:::

# Avoid Fallacy

![](img/2025-HypothesisTesting-Huber_23.png)

# The p value is the probability that the data could happen, under the condition that the null hypothesis is true.
It is not the probability that the null hypothesis is true.
Absence of evidence ‚ßß evidence of absence

# Limitations of p-value based hypothesis testing

# Summarizing the data into one single number mushes together effect size and sample size
Often, the 'null' is small (point-like), alternative is large (region-like). With enough power, even tiny effects are ‚Äòsignificant‚Äô
Correlation is not causation (confounders)
No place to take into account plausibility or 'prior' knowledge

# Don‚Äôt report absurdly small p values

![](img/2025-HypothesisTesting-Huber_24.png)

![](img/2025-HypothesisTesting-Huber_25.png)

# Reporting p values, W. Huber, Cell Systems, DOI: 10.1016/j.cels.2019.03.001

# What is p value hacking ?

# On the same data, try different tests until one is significant
On the same data, try different hypotheses until one is significant(HARKing - hypothesizing after results are known)
Moreover‚Ä¶:retrospective data picking‚Äòoutlier‚Äô removalthe 5% threshold and publication bias

<span style="color:#000000">The ASA's Statement on p\-Values: Context\, Process\, and Purpose</span>

<span style="color:#000000">Ronald L\. Wasserstein & Nicole A\. Lazara DOI: 10\.1080/00031305\.2016\.1154108</span>

<span style="color:#0096ff">What can we do about this?</span>

# The p-value is the right answer to the wrong question

# Researchers (regulators, investors, etc.) usually want to know: 
If I publish this finding (allow this drug, invest in this product, ...), what is the probability that I'll later be proven wrong  (cause harm, lose my money, ‚Ä¶)?  (a.k.a. ‚Äúfalse discovery probability‚Äù) 
The p value is:
If the finding is wrong (null hypothesis is true), what is the probability of seeing the data.
Can we compute the answer to the interesting question instead?

# Multiple Testing

![](img/2025-HypothesisTesting-Huber_26.png)

Many data analysis approaches in genomics employ item\-by\-itemtesting:

Expression profiling

Differential microbiome analysis

Genetic or chemical compound screens

Genome\-wide association studies

Proteomics

Variant calling

‚Ä¶

![](img/2025-HypothesisTesting-Huber_27.png)

![](img/2025-HypothesisTesting-Huber_28.png)

![](img/2025-HypothesisTesting-Huber_29.png)

![](img/2025-HypothesisTesting-Huber_30.png)

![](img/2025-HypothesisTesting-Huber_31.png)

![](img/2025-HypothesisTesting-Huber_32.png)

![](img/2025-HypothesisTesting-Huber_33.png)

![](img/2025-HypothesisTesting-Huber_34.png)

# False Positive Rate and False Discovery Rate

![](img/2025-HypothesisTesting-Huber_35.png)

<span style="color:#000000">FPR: fraction of FP among all true negatives</span>

<span style="color:#000000">FDR: fraction of FP among hits called</span>

<span style="color:#000000">Example:</span>

<span style="color:#000000">20\,000 genes\, 500 are d\.e\.\,100 hits called\, 10 of them wrong\.</span>

<span style="color:#000000">FPR: 10/19\,500 ‚âà 0\.05%</span>

<span style="color:#000000">FDR: 10/100 = 10%</span>

# The Multiple Testing Burden

When performing several tests\, false positive error goes up:  forŒ± = 0\.05 and  _n _ indep\. tests\, probability of no false positive result is

![](img/2025-HypothesisTesting-Huber_36.png)

![](img/2025-HypothesisTesting-Huber_37.png)

::: {.notes}

Multiple testing has sometimes been presented as a ‚Äòburden‚Äô, based on the observation that control of the FWER becomes increasingly hard as the number of tests increases.

:::

# Bonferroni Correction

![](img/2025-HypothesisTesting-Huber_38.png)

# For m tests, multiply each p-value with m.
Then see if anyone still remains below Œ±.

# The Multiple Testing Opportunity

![](img/2025-HypothesisTesting-Huber_39.png)

::: {.notes}

However, MT is also an opportunity. I like to explain this with this xkcd comic. 2 scientists have a machine that detects when sun goes nova. The machine has an error rate of 1/36. It beeps. The frequentist takes that data, computes that the type I error is 1/36<0.05‚Ä¶ Bayesian knows a bit about astrophysics, that the sun‚Äôs mass is really too small, it will only become a red giant then white dwarf then cool off. That informs her prior - and the strong prior together with the weak data lead her to conclude that no nova has happened. Empirical Bayesian doesn‚Äôt actually know any physics - but she‚Äôs been around for a while and has observed that for many thousands of times before, there was no nova. In that case, the multiple testing gives her the opportunity to better analyse the data, and even without knowing much physics.

:::

# False Discovery Rate

![](img/2025-HypothesisTesting-Huber_40.png)

<span style="color:#000000">false discoveries</span>

<span style="color:#0096ff">Method of Benjamini & Hochberg \(1995\)</span>

::: {.notes}

An important diagnostic plot in the analysis of such datasets is the p-value histogram. Uniform ‚Äòbackground‚Äô - non-d.e.; and peak of p-values at the left, d.e. genes
Red line, blue line, intuitive estimate of FDP.
This can be formalized with the concept of FDR

:::

# Method of Benjamini & Hochberg

![](img/2025-HypothesisTesting-Huber_41.png)

![](img/2025-HypothesisTesting-Huber_42.png)

::: {.notes}

In fact you don‚Äôt need to eyeball histograms to estimate or control FDR. The method of B&H essentially does what I showed you before, but in a more elegant manner. The alg. sorts the p-values‚Ä¶, last intersection. The implementation in R (function p.adjust in stats pkg) is impressively simple, just takes 4 lines of basic vector manipulations.


Note:   a = ‚Ä¶;  oa = order(a);  a[oa][order(oa)] == a

:::

# Not all Hypothesis Tests are Created Equal

![](img/2025-HypothesisTesting-Huber_43.png)

![](img/2025-HypothesisTesting-Huber_44.png)

![](img/2025-HypothesisTesting-Huber_45.png)

::: {.notes}

Practitioners of course know about these types of covariates, and for instance in microarray/RNA-seq analysis it is common to remove genes that have too little signal. In eQTL (mammalian), distance cutoffs

These slides: https://svn.ebi.ac.uk/huber/users/ignatiadis/presentation/1702-boston.tex
Use xelatex

:::

# RNA-Seq p-value histogram stratified by average read count

![](img/2025-HypothesisTesting-Huber_46.png)

![](img/2025-HypothesisTesting-Huber_47.png)

<span style="color:#000000">Problem: how to know the weights?</span>

![](img/2025-HypothesisTesting-Huber_48.png)

![](img/2025-HypothesisTesting-Huber_49.png)

<span style="color:#0096ff">Independent hypothesis weighting \(IHW\)</span>

![](img/2025-HypothesisTesting-Huber_50.png)

<span style="color:#000000">Nikos Ignatiadis</span>

<span style="color:#000000">Bioconductor package </span>  <span style="color:#000000"> __IHW__ </span>  <span style="color:#000000"> </span>

<span style="color:#000000">Ignatiadis et al\.\, </span>

<span style="color:#000000">Nature Methods 2016\, DOI 10\.1038/nmeth\.3885</span>

<span style="color:#000000">JRSSB 2021\, DOI 10\.1111/rssb\.12411</span>

<span style="color:#0096ff">RNA\-Seq example \(DESeq2\)</span>

![](img/2025-HypothesisTesting-Huber_51.png)

![](img/2025-HypothesisTesting-Huber_52.png)

# Ranking is not monotonous in raw p-values

![](img/2025-HypothesisTesting-Huber_53.png)

![](img/2025-HypothesisTesting-Huber_54.png)

# The decision boundaries is in two dimensions

![](img/2025-HypothesisTesting-Huber_55.png)

# Summary



* Multiple testing is not a problem but an opportunity
* Heterogeneity across tests
* Informative covariates are often apparent to domain scientists
  * independent of test statistic under the null
  * informative on œÄ1\, Falt
* Can do data\-driven weighting \(‚ÄúIHW‚Äù\)
  * Scales well to millions of hypotheses
  * Controls ‚Äòoveroptimism‚Äô
![](img/2025-HypothesisTesting-Huber_56.png)

<span style="color:#ffffff">The p value is not enough</span>

![](img/2025-HypothesisTesting-Huber_57.png)

<span style="color:#0096ff">Same p\-value\, different FDR / fdr</span>

![](img/2025-HypothesisTesting-Huber_58.png)

<span style="color:#000000">œÄ0 = 0\.95</span>

<span style="color:#000000">same</span>  <span style="color:#000000"> </span>  <span style="color:#000000"> _F_ </span>  <span style="color:#000000">alt</span>

<span style="color:#000000">œÄ0 = 0\.6</span>

<span style="color:#000000">different</span>  <span style="color:#000000"> </span>  <span style="color:#000000"> _F_ </span>  <span style="color:#000000">alt</span>

::: {.notes}

We can now similarly use this link to between the two-groups model and the BH procedure to understand what is going on in IHW, and for this we extend the TGM to the cond. TGM

Similarly to the two-groups model, we are just interested in it being an approximation to the truth. In practice, the critical component is the conditional independence of $X_i$ and $P_i$ under the null hypothesis. In addition, we can only expect power gains when indeed $F_{\text{alt} \mid X_i = x}$ and $\pi_0(x)$ are not constant as functions of $x$.

:::

