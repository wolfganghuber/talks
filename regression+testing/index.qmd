---
title: ""
title-slide-attributes:
  data-background-image: img/title-page-dolomites.png
  data-background-size: contain
  data-background-opacity: "1"
  data-heading-color: #E84614;
author: Wolfgang Huber
date: 2025-07-22
date-format: iso
format:
  revealjs: 
    theme: [default, wh.scss]
    transition: slide
    scrollable: true
    slide-number: c/t
    show-slide-number: all
    auto-stretch: false
    center-title-slide: false
    logo: img/ubds3-logo.png
# auto-stretch is a huge source of grief for slide layout, see https://quarto.org/docs/presentations/revealjs/advanced.html --> stretch
  html:
    code-line-numbers: false
execute: 
  echo: false
  warning: true
  error: false
  message: false
slide-level: 1
editor_options: 
  chunk_output_type: console
engine: knitr
knitr:
  opts_chunk: 
    R.options:
      width: 120
---

# Analysis of variance, regression and testing
Wolfgang Huber

![](img/title-page-dolomites.png)


# Motivation

## Test efficacy of a drug on people

- not an experiment — no complete control
- finite sample size

## Prioritise results from a biological high-throughput experiment 

- e.g., RNA-seq differential expression
- CRISPR screen

## Understand impact of humidity on prevalence of leptospirosis

::: {.incremental}
-  No understanding of mechanism involved / needed / desired 
-  Wouldn't we want to use any available understanding or ‘priors'?
:::

::: {.absolute top=100 left=200 width="800"}
![Any fool can work with infinite data. Statisticians do finity](img/sherlock.png){.fragment}
:::

# Analysis of variance

A set of observations

```{r}
#| label: two2dpoints-blank
#| echo: false
#| message: false
#| fig-width: 5
#| fig-height: 5
#| out-width: "50%"
library("tidyverse")
library("ggbeeswarm")
set.seed(0xdada)
beta = c(1, 1, 0.2)
f = function(x, gr1, gr2) (beta[3] * x + beta[1] * as.integer(gr1) + beta[2] * as.integer(gr2))

simdat = tibble(
  x = runif(100, min = 0, max = 10),
  gr0 = factor(rep("", length(x))),
  gr1 = factor(sample(c("indolent", "aggresive"), length(x), replace = TRUE)),
  gr2 = factor(sample(c("hot", "cold"), length(x), replace = TRUE)),
  y = rnorm(length(x), mean = f(x, gr1, gr2), sd = 0.3),
)
ylim = range(simdat$y)
sd0 = sd(simdat$y) |> round(1)
p = ggplot(simdat, aes(x = gr0, y = y)) + geom_boxplot() + geom_beeswarm(cex = 3) + xlab("") + ylim(ylim)
p 
```

Standard deviation: `r sd0`.

<br>

### Find an explanatory factor

```{r}
#| label: two2dpoints-onefactor
#| echo: false
#| fig-width: 5
#| fig-height: 5
#| out-width: "50%"
sd1 = group_by(simdat, gr2) |> summarise(v = var(y)) |> (\(x) round(sqrt(mean(x$v)), 1))()
p + facet_grid(cols = vars(gr2))
```

Standard deviation: `r sd1`.

<br>

### And another explanatory factor

```{r}
#| label: two2dpoints-twofactors
#| echo: false
#| fig-width: 5
#| fig-height: 5
#| out-width: "50%"
sd2 = group_by(simdat, gr1, gr2) |> summarise(v = var(y), .groups = "keep") |> (\(x) round(sqrt(mean(x$v)), 1))()
p + facet_grid(rows = vars(gr1), cols = vars(gr2))
```

Standard deviation: `r sd2`.

<br>

### And another explanatory variable, this time continuous-valued

```{r}
#| label: two2dpoints-twofactors+continuous
#| echo: false
#| fig-width: 5
#| fig-height: 5
#| out-width: "50%"
ggplot(simdat, aes(x = x, y = y)) + geom_point(aes(x = x)) + facet_grid(rows = vars(gr1), cols = vars(gr2)) + ylim(ylim)
```

Residuals: $y - \beta_1\,g1 - \beta_2\,g2 - \beta_3\,x$.

```{r}
#| label: two2dpoints-residuals
#| echo: false
#| fig-width: 5
#| fig-height: 5
#| out-width: "50%"
sd3 = with(simdat, sd(y - f(x, gr1, gr2))) |> (\(x) paste(round(x, 1), collapse = ", "))()
ggplot(simdat, aes(x = gr0, y = y - f(x, gr1, gr2))) + geom_beeswarm(cex = 3) + 
  xlab("") + ylim(diff(range(ylim))*c(-0.5,.5))
```

Standard deviation: `r sd3`.

<br>

## Basic idea of ANOVA

$$
\text{Overall variance}  = 
  \text{Signal component 1} + 
  \text{Signal component 2} + 
  \ldots + 
  \text{Rest}  
$$


# Linear model

$$
 y = \beta_0 + \beta_1\,x_1 + \beta_2\,x_2 + \ldots + \varepsilon 
$$

::: {.fragment}
In vector notation

$$
y = \beta \cdot x + \varepsilon 
$$

where now $x,\beta\in\mathbb{R}^n$ and $x = (1, x_1, x_2, \ldots)$.
:::

::: {.fragment}
If we let $\beta$ be an $m\times n$ matrix, $y$ itself can be vector-valued ($y, \varepsilon \in \mathbb{R}^m$).
:::

<br>

## But how to estimate $\beta$ ? {.fragment}

::: {.fragment}
Basic idea: minimize $\varepsilon$. 
:::

::: {.fragment}
Ordinary least squares: $\Rightarrow$
$$
\hat{\beta} = (x^t\,x)^{-1}\,x^t\,y
$$
:::

::: {.fragment}
Example: two groups, $x$ is either $(1,0)$ or $(1,1)$ $\Rightarrow$ 
 - $\hat{\beta}_0 = \text{mean of 1st group}$
 - $\hat{\beta}_1 = \text{difference between the mean of the 2nd group and that of the 1st}$.
:::

<br>

## The connection to hypothesis testing  {.fragment}

::: {.fragment}
In the two groups example: how sure are we that the __true__ $\beta_1$ is different from zero ?

This is the $t$-test!

(More generally, could ask whether $\beta$ is outside a set $N\subset\mathbb{R}^n$)


:::

# Colinearity, confounding

```{r}
#| label: colinear-1
#| echo: !expr -1
#| message: false
library("MASS")
data("Boston")
head(Boston)
```

Housing Values in Suburbs of Boston.
The `Boston` data frame has 506 rows and 14 columns.

```{r}
#| label: bostonanno
#| echo: false
bostonanno = read.table(textConnection("
varname|meaning
zn|proportion of residential land zoned for lots over 25,000 sq.ft
indus|proportion of non-retail business acres per town
chas|Charles River proximity (1 if tract bounds river; 0 otherwise)
nox|nitrogen oxides concentration (parts per 10 million)
rm|average number of rooms per dwelling
age|proportion of owner-occupied units built prior to 1940
dis|weighted mean of distances to five Boston employment centres
rad|index of accessibility to radial highways
tax|full-value property-tax rate per $10,000
ptratio|pupil-teacher ratio by town
black|$1000(b−0.63)^2$, where $b$ is the proportion of blacks
lstat|lower status of the population (percent)
medv|median value of owner-occupied homes in $1000s"), sep = "|", header = TRUE)
knitr::kable(dplyr::filter(bostonanno, varname %in% c("nox", "rm", "dis")))
```

```{r}
#| label: colinear-2
#| echo: true
#| message: false
#| fig-width: 5
#| fig-height: 5
#| out-width: "60%"
fit1 = lm(medv ~ nox + dis + rm, data = Boston)
fit2 = lm(medv ~ dis + rm, data = Boston)
coef(fit1)
coef(fit2)

library("GGally")
ggpairs(dplyr::select(Boston, nox, dis, rm))

library("corrplot")
corrplot(cor(Boston), method = "circle", type = "upper", tl.cex = 0.7)
```
```{r}
#| label: colinear-3
#| echo: false
knitr::kable(bostonanno)
```

# blocking, pairing


# Generalizations

## GLMs, Link functions

## Splines

## Gaussian processes


Yes, a **Gaussian Process (GP)** can indeed be understood as a **generalization or extension of linear regression**, especially when viewed from a **Bayesian perspective**. Here's a breakdown of the connection:

---

### 🔧 Linear Regression (Bayesian view)

In Bayesian linear regression, we assume the data is generated from a **linear function** with some Gaussian noise:

$$
y = \mathbf{x}^\top \mathbf{w} + \epsilon, \quad \epsilon \sim \mathcal{N}(0, \sigma^2)
$$

We place a **prior** on the weights:

$$
\mathbf{w} \sim \mathcal{N}(0, \tau^2 I)
$$

This leads to a **Gaussian prior** over functions $f(\mathbf{x}) = \mathbf{x}^\top \mathbf{w}$. When we marginalize over $\mathbf{w}$, the function values at input points follow a **multivariate Gaussian distribution**.

---

### 🌀 Gaussian Process as Infinite Linear Regression

A Gaussian Process can be seen as an **infinite-dimensional generalization** of Bayesian linear regression:

* Instead of explicitly parameterizing functions with a finite weight vector $\mathbf{w}$, we define a **distribution over functions** directly.
* A GP is fully specified by its **mean function** $m(\mathbf{x})$ and **covariance function (kernel)** $k(\mathbf{x}, \mathbf{x}')$.

Formally:

$$
f(\mathbf{x}) \sim \mathcal{GP}(m(\mathbf{x}), k(\mathbf{x}, \mathbf{x}'))
$$

In fact, if you choose a **linear kernel**:

$$
k(\mathbf{x}, \mathbf{x}') = \mathbf{x}^\top \mathbf{x}'
$$

then the GP reduces to Bayesian linear regression.

---

### 📈 Why GPs are more powerful

* By choosing **nonlinear kernels** (e.g. RBF, Matérn), GPs can represent **nonlinear functions** while still retaining the analytical tractability of Gaussian distributions.
* This makes them **nonparametric** models—unlike linear regression which has a fixed number of parameters, GPs can adapt complexity to the data.

---

### 🌳 Summary

| Concept       | Linear Regression                               | Gaussian Process                    |
| ------------- | ----------------------------------------------- | ----------------------------------- |
| Function form | Linear in features                              | Can be nonlinear (via kernel)       |
| Parameters    | Finite vector $\mathbf{w}$                      | Infinite-dimensional function space |
| Prior         | Over parameters                                 | Over functions                      |
| Posterior     | Gaussian over weights → predictive distribution | Gaussian over functions             |

> So yes — a Gaussian Process **extends** linear regression by replacing a finite linear model with a flexible, probabilistic model over functions.


# Fundamental tradeoffs in statistical decision making

![](img/moose.jpg) 

# Basic problem: binary decision

![value: some useful number computed from the data](img/twoclasses_z.png)

False discovery rate
$$
\text{FDR} = \frac{\text{area shaded in light blue}}{\text{sum of areas to left of vertical bar (light blue + dark red)}}
$$

For this, we need to know:

- blue curve: how is $x$ distributed if no effect
- red curve: how is $x$ distributed if there is effect
- the relative sizes of the blue and the red classes

# Basic problem: binary decision

![p value: suitably transformed version of the value from the preceding slide](img/twoclasses_p.png)

$$
\text{p} = \frac{\text{area shaded in light blue}}{\text{sum of the blue areas (=1)}}
$$

For this, we need to know:

- how is $x$ distributed if no effect

::: {.absolute top=600 left=300 width="800"}
![](img/pvalue-what-could-go-wrong.png){.fragment}
:::

# 
![](img/2025-HypothesisTesting-Huber_15.gif)
 
| Hypothesis testing                              | Machine Learning                                |
| ----------------------------------------------- | ----------------------------------------------- | 
| Some theory/model and no or few parameters | Lots of free parameters | 
| No training data | Lots of training data |
| More rigid/formulaic | Using multiple variables |
| Regulatory use | ... or objects that are not even traditional variables (kernel methods, DNN) |


# Comparing a univariate ($\in \mathbb{R}$) measurement between two groups: the $t$-statistic

![](img/2025-HypothesisTesting-Huber_16.png)

![](img/2025-HypothesisTesting-Huber_17.png)

![](img/2025-HypothesisTesting-Huber_18.png)

# Parametric Theory vs Simulation

![](img/2025-HypothesisTesting-Huber_19.png)

![](img/2025-HypothesisTesting-Huber_20.png)

<span style="color:#0096ff"> __Q__ </span>  <span style="color:#000000">:</span>

<span style="color:#000000">Discuss pros and contras for each</span>

![](img/2025-HypothesisTesting-Huber_21.jpg)

![](img/2025-HypothesisTesting-Huber_22.png)

# Common misunderstandings about t-statistic and t-distribution

# “If the data with sample sizes n1 and n2 are identically normal distributed and independent,  then the t-test is optimal, and under H0, the t-statistic follows a t-distribution with ν=n1+n2-2.”
These are sufficient, not necessary, conditions.
Deviation from normality: test typically maintains FPR (“type-I error”) control. (It just no longer has provably optimal power.)Options: transform data, use permutations, simulations 
Deviation from independence: FPR control is lost, p-values will likely be totally wrong (e.g., for positive correlation, too optimistic).No easy options: … simulations that keep the dependence structure (?)… batch adjustments that remove correlation (?)… empirical null — possible if you have many tests

::: {.notes}

Note that the null hypothesis, and the power of Wilcoxon test are different from the t-test

:::

# Avoid Fallacy

![](img/2025-HypothesisTesting-Huber_23.png)

# The p value is the probability that the data could happen, under the condition that the null hypothesis is true.
It is not the probability that the null hypothesis is true.
Absence of evidence ⧧ evidence of absence

# Limitations of p-value based hypothesis testing

# Summarizing the data into one single number mushes together effect size and sample size
Often, the 'null' is small (point-like), alternative is large (region-like). With enough power, even tiny effects are ‘significant’
Correlation is not causation (confounders)
No place to take into account plausibility or 'prior' knowledge

# Don’t report absurdly small p values

![](img/2025-HypothesisTesting-Huber_24.png)

![](img/2025-HypothesisTesting-Huber_25.png)

# Reporting p values, W. Huber, Cell Systems, DOI: 10.1016/j.cels.2019.03.001

# What is p value hacking ?

# On the same data, try different tests until one is significant
On the same data, try different hypotheses until one is significant(HARKing - hypothesizing after results are known)
Moreover…:retrospective data picking‘outlier’ removalthe 5% threshold and publication bias

<span style="color:#000000">The ASA's Statement on p\-Values: Context\, Process\, and Purpose</span>

<span style="color:#000000">Ronald L\. Wasserstein & Nicole A\. Lazara DOI: 10\.1080/00031305\.2016\.1154108</span>

<span style="color:#0096ff">What can we do about this?</span>

# The p-value is the right answer to the wrong question

# Researchers (regulators, investors, etc.) usually want to know: 
If I publish this finding (allow this drug, invest in this product, ...), what is the probability that I'll later be proven wrong  (cause harm, lose my money, …)?  (a.k.a. “false discovery probability”) 
The p value is:
If the finding is wrong (null hypothesis is true), what is the probability of seeing the data.
Can we compute the answer to the interesting question instead?

# Multiple Testing

![](img/2025-HypothesisTesting-Huber_26.png)

Many data analysis approaches in genomics employ item\-by\-itemtesting:

Expression profiling

Differential microbiome analysis

Genetic or chemical compound screens

Genome\-wide association studies

Proteomics

Variant calling

…

![](img/2025-HypothesisTesting-Huber_27.png)

![](img/2025-HypothesisTesting-Huber_28.png)

![](img/2025-HypothesisTesting-Huber_29.png)

![](img/2025-HypothesisTesting-Huber_30.png)

![](img/2025-HypothesisTesting-Huber_31.png)

![](img/2025-HypothesisTesting-Huber_32.png)

![](img/2025-HypothesisTesting-Huber_33.png)

![](img/2025-HypothesisTesting-Huber_34.png)

# False Positive Rate and False Discovery Rate

![](img/2025-HypothesisTesting-Huber_35.png)

<span style="color:#000000">FPR: fraction of FP among all true negatives</span>

<span style="color:#000000">FDR: fraction of FP among hits called</span>

<span style="color:#000000">Example:</span>

<span style="color:#000000">20\,000 genes\, 500 are d\.e\.\,100 hits called\, 10 of them wrong\.</span>

<span style="color:#000000">FPR: 10/19\,500 ≈ 0\.05%</span>

<span style="color:#000000">FDR: 10/100 = 10%</span>

# The Multiple Testing Burden

When performing several tests\, false positive error goes up:  forα = 0\.05 and  _n _ indep\. tests\, probability of no false positive result is

![](img/2025-HypothesisTesting-Huber_36.png)

![](img/2025-HypothesisTesting-Huber_37.png)

::: {.notes}

Multiple testing has sometimes been presented as a ‘burden’, based on the observation that control of the FWER becomes increasingly hard as the number of tests increases.

:::

# Bonferroni Correction

![](img/2025-HypothesisTesting-Huber_38.png)

# For m tests, multiply each p-value with m.
Then see if anyone still remains below α.

# The Multiple Testing Opportunity

![](img/2025-HypothesisTesting-Huber_39.png)

::: {.notes}

However, MT is also an opportunity. I like to explain this with this xkcd comic. 2 scientists have a machine that detects when sun goes nova. The machine has an error rate of 1/36. It beeps. The frequentist takes that data, computes that the type I error is 1/36<0.05… Bayesian knows a bit about astrophysics, that the sun’s mass is really too small, it will only become a red giant then white dwarf then cool off. That informs her prior - and the strong prior together with the weak data lead her to conclude that no nova has happened. Empirical Bayesian doesn’t actually know any physics - but she’s been around for a while and has observed that for many thousands of times before, there was no nova. In that case, the multiple testing gives her the opportunity to better analyse the data, and even without knowing much physics.

:::

# False Discovery Rate

![](img/2025-HypothesisTesting-Huber_40.png)

<span style="color:#000000">false discoveries</span>

<span style="color:#0096ff">Method of Benjamini & Hochberg \(1995\)</span>

::: {.notes}

An important diagnostic plot in the analysis of such datasets is the p-value histogram. Uniform ‘background’ - non-d.e.; and peak of p-values at the left, d.e. genes
Red line, blue line, intuitive estimate of FDP.
This can be formalized with the concept of FDR

:::

# Method of Benjamini & Hochberg

![](img/2025-HypothesisTesting-Huber_41.png)

![](img/2025-HypothesisTesting-Huber_42.png)

::: {.notes}

In fact you don’t need to eyeball histograms to estimate or control FDR. The method of B&H essentially does what I showed you before, but in a more elegant manner. The alg. sorts the p-values…, last intersection. The implementation in R (function p.adjust in stats pkg) is impressively simple, just takes 4 lines of basic vector manipulations.


Note:   a = …;  oa = order(a);  a[oa][order(oa)] == a

:::

# Not all Hypothesis Tests are Created Equal

![](img/2025-HypothesisTesting-Huber_43.png)

![](img/2025-HypothesisTesting-Huber_44.png)

![](img/2025-HypothesisTesting-Huber_45.png)

::: {.notes}

Practitioners of course know about these types of covariates, and for instance in microarray/RNA-seq analysis it is common to remove genes that have too little signal. In eQTL (mammalian), distance cutoffs

These slides: https://svn.ebi.ac.uk/huber/users/ignatiadis/presentation/1702-boston.tex
Use xelatex

:::

# RNA-Seq p-value histogram stratified by average read count

![](img/2025-HypothesisTesting-Huber_46.png)

![](img/2025-HypothesisTesting-Huber_47.png)

<span style="color:#000000">Problem: how to know the weights?</span>

![](img/2025-HypothesisTesting-Huber_48.png)

![](img/2025-HypothesisTesting-Huber_49.png)

<span style="color:#0096ff">Independent hypothesis weighting \(IHW\)</span>

![](img/2025-HypothesisTesting-Huber_50.png)

<span style="color:#000000">Nikos Ignatiadis</span>

<span style="color:#000000">Bioconductor package </span>  <span style="color:#000000"> __IHW__ </span>  <span style="color:#000000"> </span>

<span style="color:#000000">Ignatiadis et al\.\, </span>

<span style="color:#000000">Nature Methods 2016\, DOI 10\.1038/nmeth\.3885</span>

<span style="color:#000000">JRSSB 2021\, DOI 10\.1111/rssb\.12411</span>

<span style="color:#0096ff">RNA\-Seq example \(DESeq2\)</span>

![](img/2025-HypothesisTesting-Huber_51.png)

![](img/2025-HypothesisTesting-Huber_52.png)

# Ranking is not monotonous in raw p-values

![](img/2025-HypothesisTesting-Huber_53.png)

![](img/2025-HypothesisTesting-Huber_54.png)

# The decision boundaries is in two dimensions

![](img/2025-HypothesisTesting-Huber_55.png)

# Summary



* Multiple testing is not a problem but an opportunity
* Heterogeneity across tests
* Informative covariates are often apparent to domain scientists
  * independent of test statistic under the null
  * informative on π1\, Falt
* Can do data\-driven weighting \(“IHW”\)
  * Scales well to millions of hypotheses
  * Controls ‘overoptimism’
![](img/2025-HypothesisTesting-Huber_56.png)

<span style="color:#ffffff">The p value is not enough</span>

![](img/2025-HypothesisTesting-Huber_57.png)

<span style="color:#0096ff">Same p\-value\, different FDR / fdr</span>

![](img/2025-HypothesisTesting-Huber_58.png)

<span style="color:#000000">π0 = 0\.95</span>

<span style="color:#000000">same</span>  <span style="color:#000000"> </span>  <span style="color:#000000"> _F_ </span>  <span style="color:#000000">alt</span>

<span style="color:#000000">π0 = 0\.6</span>

<span style="color:#000000">different</span>  <span style="color:#000000"> </span>  <span style="color:#000000"> _F_ </span>  <span style="color:#000000">alt</span>

::: {.notes}

We can now similarly use this link to between the two-groups model and the BH procedure to understand what is going on in IHW, and for this we extend the TGM to the cond. TGM

Similarly to the two-groups model, we are just interested in it being an approximation to the truth. In practice, the critical component is the conditional independence of $X_i$ and $P_i$ under the null hypothesis. In addition, we can only expect power gains when indeed $F_{\text{alt} \mid X_i = x}$ and $\pi_0(x)$ are not constant as functions of $x$.

:::

